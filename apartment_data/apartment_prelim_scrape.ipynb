{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURL query: http://web.archive.org/cdx/search/cdx?url=https://www.zillow.com/berkeley-ca/apartments/&output=json\n",
    "# This script is used to scrape the archived webpages of Zillow's Berkeley apartment listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readable Timestamp: 2024-08-11\n"
     ]
    }
   ],
   "source": [
    "def convert_timestamp(timestamp):\n",
    "    \"\"\"Convert a timestamp in 'YYYYMMDDHHMMSS' format to a readable string.\"\"\"\n",
    "    try:\n",
    "        readable_format = datetime.strptime(timestamp, \"%Y%m%d%H%M%S\").strftime(\"%Y-%m-%d\")\n",
    "        return readable_format\n",
    "    except ValueError:\n",
    "        return \"Invalid timestamp format. Please use 'YYYYMMDDHHMMSS'.\"\n",
    "\n",
    "# Example usage\n",
    "timestamp = \"20240811231741\"\n",
    "print(f\"Readable Timestamp: {convert_timestamp(timestamp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"apartment_snapshots.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Skip the header row and construct the archive URLs\n",
    "timestamps = [\n",
    "    convert_timestamp(entry[1]) for entry in data[1:] if convert_timestamp(entry[1]) > \"2020\"\n",
    "]\n",
    "\n",
    "urls = [\n",
    "    f\"http://web.archive.org/web/{entry[1]}/{entry[2]}\"\n",
    "    for entry in data[1:] if convert_timestamp(entry[1]) > \"2020\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(urls), len(timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save tables\n",
    "if not os.path.exists(\"scraped_tables/zillow_apartments\"):\n",
    "    os.makedirs(\"scraped_tables/zillow_apartments\")\n",
    "    \n",
    "# Rate limit settings\n",
    "RATE_LIMIT = 5  # Time in seconds between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property_data_pre_2022(soup, date):\n",
    "    \"\"\"\n",
    "    Extracts property data from Zillow using BeautifulSoup.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the webpage.\n",
    "        date (str): The date of the data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the extracted property data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all the listings on the page\n",
    "    listings = soup.find_all('div', class_='list-card-heading')\n",
    "    addresses = soup.find_all(attrs={'class': 'list-card-addr'})\n",
    "    if len(listings) != len(addresses):\n",
    "        print(\"potential mismatch of listings and addresses:\", date, len(listings), len(addresses))\n",
    "\n",
    "    # Loop through each listing div and extract details\n",
    "    property_data = []\n",
    "    for n, listing in enumerate(listings):\n",
    "        # Extract price\n",
    "        price = listing.find('div', class_='list-card-price')\n",
    "        price_text = price.get_text(strip=True) if price else None\n",
    "        if not price_text:\n",
    "            continue\n",
    "\n",
    "        # Extract details\n",
    "        details = listing.find('ul', class_='list-card-details')\n",
    "        details_items = details.find_all('li') if details else []\n",
    "\n",
    "        # Initialize variables for type, bathrooms, and size\n",
    "        property_type = details_items[0].get_text(strip=True) if len(details_items) > 0 else None\n",
    "        bathrooms = details_items[1].get_text(strip=True) if len(details_items) > 1 else None\n",
    "        size = details_items[2].get_text(strip=True) if len(details_items) > 2 else None\n",
    "\n",
    "        # Create a dictionary for the current listing\n",
    "        property_info = {\n",
    "            \"price\": price_text,\n",
    "            \"type\": property_type,\n",
    "            \"bathrooms\": bathrooms,\n",
    "            \"size\": size,\n",
    "            \"address\": addresses[n].get_text(strip=True),\n",
    "            \"date\": date\n",
    "        }\n",
    "        property_data.append(property_info)\n",
    "    return property_data\n",
    "\n",
    "def get_property_data_post_2022(soup, date):\n",
    "    \"\"\"\n",
    "    Extracts property data from Zillow using BeautifulSoup.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the webpage.\n",
    "        date (str): The date of the data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the extracted property data.\n",
    "    \"\"\"\n",
    "    \n",
    "    property_data = []\n",
    "    # Find all the addresses on the page\n",
    "    # addresses = soup.find_all('address', attrs={'data-test': 'property-card-addr'})\n",
    "    listing_groups = soup.find_all('div', class_='property-card-data')\n",
    "\n",
    "    for listing_soup in listing_groups:\n",
    "        address = listing_soup.find('address', attrs={'data-test': 'property-card-addr'}).get_text(strip=True)\n",
    "        \n",
    "        # Headline data\n",
    "        header_price_data = listing_soup.find('span', attrs={'data-test': 'property-card-price'}).get_text(strip=True)\n",
    "        property_data.append({\n",
    "            \"price\": header_price_data.split(' ')[0],\n",
    "            \"type\": header_price_data[header_price_data.index(' ')+1:] if ' ' in header_price_data else None,\n",
    "            \"bathrooms\": None,\n",
    "            \"size\": None,\n",
    "            \"address\": address,\n",
    "            \"date\": date\n",
    "        })\n",
    "\n",
    "        # Other room data\n",
    "        other_rooms_section = listing_soup.find_all('span', class_='jlVIIO')\n",
    "        for container in other_rooms_section:\n",
    "            for room in container.find_all('span', recursive=False):\n",
    "                bold_text = room.find('b')\n",
    "                if bold_text:\n",
    "                    # Extract the price from the <b> tag\n",
    "                    price = bold_text.get_text(strip=True)\n",
    "                    room_type = room.get_text(strip=True).replace(price, '').strip()\n",
    "                    property_data.append({\n",
    "                        \"price\": price,\n",
    "                        \"type\": room_type,\n",
    "                        \"bathrooms\": None,\n",
    "                        \"size\": None,\n",
    "                        \"address\": address,\n",
    "                        \"date\": date\n",
    "                    })\n",
    "        \n",
    "        other_rooms_section = listing_soup.find_all('ul', class_='dmDolk')\n",
    "        for container in other_rooms_section:\n",
    "            for room in container.find_all('li', recursive=False):\n",
    "                bold_text = room.find('b')\n",
    "                if bold_text:\n",
    "                    # Extract the price from the <b> tag\n",
    "                    price = bold_text.get_text(strip=True)\n",
    "                    room_type = room.get_text(strip=True).replace(price, '').strip()\n",
    "                    property_data.append({\n",
    "                        \"price\": price,\n",
    "                        \"type\": room_type,\n",
    "                        \"bathrooms\": None,\n",
    "                        \"size\": None,\n",
    "                        \"address\": address,\n",
    "                        \"date\": date\n",
    "                    })\n",
    "    \n",
    "    return property_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_info_from_site(url, date):\n",
    "    \"\"\"\n",
    "    Scrapes information from a Zillow URL and saves it to a CSV file.\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "        date (str): The date in the format \"YYYY-MM-DD\".\n",
    "    Raises:\n",
    "        HTTPError: If the request to the website fails.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    url.strip()\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch URL: {url}\")\n",
    "        return\n",
    "\n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.content, 'html')\n",
    "\n",
    "    # Find detailed property information\n",
    "    # Zillow changed frontend UI starting after url[89]: 2022-05-14\n",
    "    if date < \"2022-05-14\":\n",
    "        property_data = get_property_data_pre_2022(soup, date)\n",
    "    elif date == \"2022-05-14\":\n",
    "        return # skip url[89] since it's a transition date -- soup response is bugged\n",
    "    else:\n",
    "        property_data = get_property_data_post_2022(soup, date)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    filename = f\"scraped_tables/zillow_apartments/{date}.csv\"\n",
    "    df = pd.DataFrame(property_data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved table to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch URL: http://web.archive.org/web/20240716230438/https://www.zillow.com/berkeley-ca/apartments/\n"
     ]
    }
   ],
   "source": [
    "for n, url in enumerate(urls):\n",
    "    # In case of rate limiting interuptions:\n",
    "    if timestamps[n] <= \"2024-07-08\": # CHANGE THIS DATE TO LAST SUCCESSFUL SCRAPE!\n",
    "        continue\n",
    "    if timestamps[n] in [\"2023-03-14\", \"2024-07-08\"]: # these specific dates were manually scraped\n",
    "        continue\n",
    "    scrape_info_from_site(url, timestamps[n])\n",
    "    time.sleep(RATE_LIMIT)  # Rate limit to avoid overwhelming the server"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
